"use strict";
var __decorate = (this && this.__decorate) || function (decorators, target, key, desc) {
    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;
    if (typeof Reflect === "object" && typeof Reflect.decorate === "function") r = Reflect.decorate(decorators, target, key, desc);
    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;
    return c > 3 && r && Object.defineProperty(target, key, r), r;
};
var __metadata = (this && this.__metadata) || function (k, v) {
    if (typeof Reflect === "object" && typeof Reflect.metadata === "function") return Reflect.metadata(k, v);
};
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.ImportService = void 0;
const backend_common_1 = require("@n8n/backend-common");
const db_1 = require("@n8n/db");
const typeorm_1 = require("@n8n/typeorm");
const di_1 = require("@n8n/di");
const uuid_1 = require("uuid");
const promises_1 = require("fs/promises");
const path_1 = __importDefault(require("path"));
const workflow_helpers_1 = require("../workflow-helpers");
const validate_database_type_1 = require("../utils/validate-database-type");
let ImportService = class ImportService {
    constructor(logger, credentialsRepository, tagRepository, dataSource) {
        this.logger = logger;
        this.credentialsRepository = credentialsRepository;
        this.tagRepository = tagRepository;
        this.dataSource = dataSource;
        this.dbCredentials = [];
        this.dbTags = [];
        this.foreignKeyCommands = {
            disable: {
                sqlite: 'PRAGMA defer_foreign_keys = ON;',
                'sqlite-pooled': 'PRAGMA defer_foreign_keys = ON;',
                'sqlite-memory': 'PRAGMA defer_foreign_keys = ON;',
                postgres: 'SET session_replication_role = replica;',
                postgresql: 'SET session_replication_role = replica;',
            },
            enable: {
                sqlite: 'PRAGMA defer_foreign_keys = OFF;',
                'sqlite-pooled': 'PRAGMA defer_foreign_keys = OFF;',
                'sqlite-memory': 'PRAGMA defer_foreign_keys = OFF;',
                postgres: 'SET session_replication_role = DEFAULT;',
                postgresql: 'SET session_replication_role = DEFAULT;',
            },
        };
    }
    async initRecords() {
        this.dbCredentials = await this.credentialsRepository.find();
        this.dbTags = await this.tagRepository.find();
    }
    async importWorkflows(workflows, projectId) {
        await this.initRecords();
        for (const workflow of workflows) {
            workflow.nodes.forEach((node) => {
                this.toNewCredentialFormat(node);
                if (!node.id)
                    node.id = (0, uuid_1.v4)();
            });
            const hasInvalidCreds = workflow.nodes.some((node) => !node.credentials?.id);
            if (hasInvalidCreds)
                await this.replaceInvalidCreds(workflow);
        }
        const { manager: dbManager } = this.credentialsRepository;
        await dbManager.transaction(async (tx) => {
            for (const workflow of workflows) {
                if (workflow.active) {
                    workflow.active = false;
                    this.logger.info(`Deactivating workflow "${workflow.name}". Remember to activate later.`);
                }
                const exists = workflow.id ? await tx.existsBy(db_1.WorkflowEntity, { id: workflow.id }) : false;
                const upsertResult = await tx.upsert(db_1.WorkflowEntity, workflow, ['id']);
                const workflowId = upsertResult.identifiers.at(0)?.id;
                const personalProject = await tx.findOneByOrFail(db_1.Project, { id: projectId });
                if (!exists) {
                    await tx.upsert(db_1.SharedWorkflow, { workflowId, projectId: personalProject.id, role: 'workflow:owner' }, ['workflowId', 'projectId']);
                }
                if (!workflow.tags?.length)
                    continue;
                await this.tagRepository.setTags(tx, this.dbTags, workflow);
                for (const tag of workflow.tags) {
                    await tx.upsert(db_1.WorkflowTagMapping, { tagId: tag.id, workflowId }, [
                        'tagId',
                        'workflowId',
                    ]);
                }
            }
        });
    }
    async replaceInvalidCreds(workflow) {
        try {
            await (0, workflow_helpers_1.replaceInvalidCredentials)(workflow);
        }
        catch (e) {
            this.logger.error('Failed to replace invalid credential', { error: e });
        }
    }
    async isTableEmpty(tableName) {
        try {
            const result = await this.dataSource
                .createQueryBuilder()
                .select('1')
                .from(tableName, tableName)
                .limit(1)
                .getRawMany();
            this.logger.debug(`Table ${tableName} has ${result.length} rows`);
            return result.length === 0;
        }
        catch (error) {
            this.logger.error(`Failed to check if table ${tableName} is empty:`, { error });
            throw new Error(`Unable to check table ${tableName}`);
        }
    }
    async areAllEntityTablesEmpty(tableNames) {
        if (tableNames.length === 0) {
            this.logger.info('No table names provided, considering all tables empty');
            return true;
        }
        this.logger.info(`Checking if ${tableNames.length} tables are empty...`);
        const nonEmptyTables = [];
        for (const tableName of tableNames) {
            const isEmpty = await this.isTableEmpty(tableName);
            if (!isEmpty) {
                nonEmptyTables.push(tableName);
            }
        }
        if (nonEmptyTables.length > 0) {
            this.logger.info(`üìä Found ${nonEmptyTables.length} table(s) with existing data: ${nonEmptyTables.join(', ')}`);
            return false;
        }
        this.logger.info('‚úÖ All tables are empty');
        return true;
    }
    async truncateEntityTable(tableName, transactionManager) {
        this.logger.info(`üóëÔ∏è  Truncating table: ${tableName}`);
        await transactionManager.createQueryBuilder().delete().from(tableName, tableName).execute();
        this.logger.info(`   ‚úÖ Table ${tableName} truncated successfully`);
    }
    async getImportMetadata(inputDir) {
        const files = await (0, promises_1.readdir)(inputDir);
        const entityTableNamesMap = {};
        const entityFiles = {};
        for (const file of files) {
            if (file.endsWith('.jsonl')) {
                const entityName = file.replace(/\.\d+\.jsonl$/, '.jsonl').replace('.jsonl', '');
                if (entityName === 'migrations') {
                    continue;
                }
                if (!entityTableNamesMap[entityName]) {
                    const entityMetadata = this.dataSource.entityMetadatas.find((meta) => meta.name.toLowerCase() === entityName);
                    if (!entityMetadata) {
                        this.logger.warn(`‚ö†Ô∏è  No entity metadata found for ${entityName}, skipping...`);
                        continue;
                    }
                    entityTableNamesMap[entityName] = entityMetadata.tableName;
                }
                if (!entityFiles[entityName]) {
                    entityFiles[entityName] = [];
                }
                entityFiles[entityName].push(path_1.default.join(inputDir, file));
            }
        }
        return {
            tableNames: Object.values(entityTableNamesMap),
            entityFiles,
        };
    }
    async readEntityFile(filePath) {
        const content = await (0, promises_1.readFile)(filePath, 'utf8');
        const lines = content.split(/\r?\n/);
        const entities = [];
        for (let i = 0; i < lines.length; i++) {
            const line = lines[i].trim();
            if (!line)
                continue;
            try {
                entities.push(JSON.parse(line));
            }
            catch (error) {
                this.logger.error(`Failed to parse JSON on line ${i + 1} in ${filePath}`, { error });
                this.logger.error(`Line content (first 200 chars): ${line.substring(0, 200)}...`);
                throw new Error(`Invalid JSON on line ${i + 1} in file ${filePath}. JSONL format requires one complete JSON object per line.`);
            }
        }
        return entities;
    }
    async importEntities(inputDir, truncateTables) {
        (0, validate_database_type_1.validateDbTypeForImportEntities)(this.dataSource.options.type);
        await this.validateMigrations(inputDir);
        await this.dataSource.transaction(async (transactionManager) => {
            await this.disableForeignKeyConstraints(transactionManager);
            const importMetadata = await this.getImportMetadata(inputDir);
            const { tableNames, entityFiles } = importMetadata;
            const entityNames = Object.keys(entityFiles);
            if (truncateTables) {
                this.logger.info('\nüóëÔ∏è  Truncating tables before import...');
                this.logger.info(`Found ${tableNames.length} tables to truncate: ${tableNames.join(', ')}`);
                await Promise.all(tableNames.map(async (tableName) => await this.truncateEntityTable(tableName, transactionManager)));
                this.logger.info('‚úÖ All tables truncated successfully');
            }
            if (!truncateTables && !(await this.areAllEntityTablesEmpty(tableNames))) {
                this.logger.info('\nüóëÔ∏è  Not all tables are empty, skipping import, you can use --truncateTables to truncate tables before import if needed');
                return;
            }
            await this.importEntitiesFromFiles(inputDir, transactionManager, entityNames, entityFiles);
            await this.enableForeignKeyConstraints(transactionManager);
        });
    }
    async importEntitiesFromFiles(inputDir, transactionManager, entityNames, entityFiles) {
        this.logger.info(`\nüöÄ Starting entity import from directory: ${inputDir}`);
        if (entityNames.length === 0) {
            this.logger.warn('No entity files found in input directory');
            return;
        }
        this.logger.info(`üìã Found ${entityNames.length} entity types to import:`);
        let totalEntitiesImported = 0;
        await Promise.all(entityNames.map(async (entityName) => {
            const files = entityFiles[entityName];
            this.logger.info(`   ‚Ä¢ ${entityName}: ${files.length} file(s)`);
            this.logger.info(`\nüìä Importing ${entityName} entities...`);
            const entityMetadata = this.dataSource.entityMetadatas.find((meta) => meta.name.toLowerCase() === entityName);
            if (!entityMetadata) {
                this.logger.warn(`   ‚ö†Ô∏è  No entity metadata found for ${entityName}, skipping...`);
                return;
            }
            const tableName = entityMetadata.tableName;
            this.logger.info(`   üìã Target table: ${tableName}`);
            let entityCount = 0;
            await Promise.all(files.map(async (filePath) => {
                this.logger.info(`   üìÅ Reading file: ${path_1.default.basename(filePath)}`);
                const entities = await this.readEntityFile(filePath);
                this.logger.info(`      Found ${entities.length} entities`);
                if (entities.length > 0) {
                    await transactionManager.insert(tableName, entities);
                    entityCount += entities.length;
                }
            }));
            this.logger.info(`   ‚úÖ Completed ${entityName}: ${entityCount} entities imported`);
            totalEntitiesImported += entityCount;
        }));
        this.logger.info('\nüìä Import Summary:');
        this.logger.info(`   Total entities imported: ${totalEntitiesImported}`);
        this.logger.info(`   Entity types processed: ${entityNames.length}`);
        this.logger.info('‚úÖ Import completed successfully!');
    }
    toNewCredentialFormat(node) {
        if (!node.credentials)
            return;
        for (const [type, name] of Object.entries(node.credentials)) {
            if (typeof name !== 'string')
                continue;
            const nodeCredential = { id: null, name };
            const match = this.dbCredentials.find((c) => c.name === name && c.type === type);
            if (match)
                nodeCredential.id = match.id;
            node.credentials[type] = nodeCredential;
        }
    }
    async disableForeignKeyConstraints(transactionManager) {
        const disableCommand = this.foreignKeyCommands.disable[this.dataSource.options.type];
        if (!disableCommand) {
            throw new Error(`Unsupported database type: ${this.dataSource.options.type}. Supported types: sqlite, postgres`);
        }
        this.logger.debug(`Executing: ${disableCommand}`);
        await transactionManager.query(disableCommand);
        this.logger.info('‚úÖ Foreign key constraints disabled');
    }
    async enableForeignKeyConstraints(transactionManager) {
        const enableCommand = this.foreignKeyCommands.enable[this.dataSource.options.type];
        if (!enableCommand) {
            throw new Error(`Unsupported database type: ${this.dataSource.options.type}. Supported types: sqlite, postgres`);
        }
        this.logger.debug(`Executing: ${enableCommand}`);
        await transactionManager.query(enableCommand);
        this.logger.info('‚úÖ Foreign key constraints re-enabled');
    }
    async validateMigrations(inputDir) {
        const migrationsFilePath = path_1.default.join(inputDir, 'migrations.jsonl');
        try {
            await (0, promises_1.readFile)(migrationsFilePath, 'utf8');
        }
        catch (error) {
            throw new Error('Migrations file not found. Cannot proceed with import without migration validation.');
        }
        const migrationsFileContent = await (0, promises_1.readFile)(migrationsFilePath, 'utf8');
        const importMigrations = migrationsFileContent
            .trim()
            .split('\n')
            .filter((line) => line.trim())
            .map((line) => {
            try {
                return JSON.parse(line);
            }
            catch (error) {
                throw new Error(`Invalid JSON in migrations file: ${error instanceof Error ? error.message : 'Unknown error'}`);
            }
        });
        if (importMigrations.length === 0) {
            this.logger.info('No migrations found in import data');
            return;
        }
        const latestImportMigration = importMigrations.reduce((latest, current) => {
            const currentTimestamp = parseInt(String(current.timestamp || current.id || '0'));
            const latestTimestamp = parseInt(String(latest.timestamp || latest.id || '0'));
            return currentTimestamp > latestTimestamp ? current : latest;
        });
        this.logger.info(`Latest migration in import data: ${String(latestImportMigration.name)} (timestamp: ${String(latestImportMigration.timestamp || latestImportMigration.id)}, id: ${String(latestImportMigration.id)})`);
        const tablePrefix = this.dataSource.options.entityPrefix || '';
        const migrationsTableName = `${tablePrefix}migrations`;
        const dbMigrations = await this.dataSource.query(`SELECT * FROM ${this.dataSource.driver.escape(migrationsTableName)} ORDER BY timestamp DESC LIMIT 1`);
        if (dbMigrations.length === 0) {
            throw new Error('Target database has no migrations. Cannot import data from a different migration state.');
        }
        const latestDbMigration = dbMigrations[0];
        this.logger.info(`Latest migration in target database: ${latestDbMigration.name} (timestamp: ${latestDbMigration.timestamp}, id: ${latestDbMigration.id})`);
        const importTimestamp = parseInt(String(latestImportMigration.timestamp || latestImportMigration.id || '0'));
        const dbTimestamp = parseInt(String(latestDbMigration.timestamp || '0'));
        const importName = latestImportMigration.name;
        const dbName = latestDbMigration.name;
        const importId = latestImportMigration.id;
        const dbId = latestDbMigration.id;
        if (importTimestamp !== dbTimestamp) {
            throw new Error(`Migration timestamp mismatch. Import data: ${String(importName)} (${String(importTimestamp)}) does not match target database ${String(dbName)} (${String(dbTimestamp)}). Cannot import data from different migration states.`);
        }
        if (importName !== dbName) {
            throw new Error(`Migration name mismatch. Import data: ${String(importName)} does not match target database ${String(dbName)}. Cannot import data from different migration states.`);
        }
        if (importId && dbId && importId !== dbId) {
            throw new Error(`Migration ID mismatch. Import data: ${String(importName)} (id: ${String(importId)}) does not match target database ${String(dbName)} (id: ${String(dbId)}). Cannot import data from different migration states.`);
        }
        this.logger.info('‚úÖ Migration validation passed - import data matches target database migration state');
    }
};
exports.ImportService = ImportService;
exports.ImportService = ImportService = __decorate([
    (0, di_1.Service)(),
    __metadata("design:paramtypes", [backend_common_1.Logger,
        db_1.CredentialsRepository,
        db_1.TagRepository,
        typeorm_1.DataSource])
], ImportService);
//# sourceMappingURL=import.service.js.map