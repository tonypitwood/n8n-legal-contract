"use strict";
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);
var LmChatAnthropic_node_exports = {};
__export(LmChatAnthropic_node_exports, {
  LmChatAnthropic: () => LmChatAnthropic
});
module.exports = __toCommonJS(LmChatAnthropic_node_exports);
var import_anthropic = require("@langchain/anthropic");
var import_httpProxyAgent = require("../../../utils/httpProxyAgent");
var import_sharedFields = require("../../../utils/sharedFields");
var import_n8n_workflow = require("n8n-workflow");
var import_n8nLlmFailedAttemptHandler = require("../n8nLlmFailedAttemptHandler");
var import_N8nLlmTracing = require("../N8nLlmTracing");
var import_searchModels = require("./methods/searchModels");
const modelField = {
  displayName: "Model",
  name: "model",
  type: "options",
  // eslint-disable-next-line n8n-nodes-base/node-param-options-type-unsorted-items
  options: [
    {
      name: "Claude 3.5 Sonnet(20241022)",
      value: "claude-3-5-sonnet-20241022"
    },
    {
      name: "Claude 3 Opus(20240229)",
      value: "claude-3-opus-20240229"
    },
    {
      name: "Claude 3.5 Sonnet(20240620)",
      value: "claude-3-5-sonnet-20240620"
    },
    {
      name: "Claude 3 Sonnet(20240229)",
      value: "claude-3-sonnet-20240229"
    },
    {
      name: "Claude 3.5 Haiku(20241022)",
      value: "claude-3-5-haiku-20241022"
    },
    {
      name: "Claude 3 Haiku(20240307)",
      value: "claude-3-haiku-20240307"
    },
    {
      name: "LEGACY: Claude 2",
      value: "claude-2"
    },
    {
      name: "LEGACY: Claude 2.1",
      value: "claude-2.1"
    },
    {
      name: "LEGACY: Claude Instant 1.2",
      value: "claude-instant-1.2"
    },
    {
      name: "LEGACY: Claude Instant 1",
      value: "claude-instant-1"
    }
  ],
  description: 'The model which will generate the completion. <a href="https://docs.anthropic.com/claude/docs/models-overview">Learn more</a>.',
  default: "claude-2"
};
const MIN_THINKING_BUDGET = 1024;
const DEFAULT_MAX_TOKENS = 4096;
class LmChatAnthropic {
  constructor() {
    this.methods = {
      listSearch: {
        searchModels: import_searchModels.searchModels
      }
    };
    this.description = {
      displayName: "Anthropic Chat Model",
      name: "lmChatAnthropic",
      icon: "file:anthropic.svg",
      group: ["transform"],
      version: [1, 1.1, 1.2, 1.3],
      defaultVersion: 1.3,
      description: "Language Model Anthropic",
      defaults: {
        name: "Anthropic Chat Model"
      },
      codex: {
        categories: ["AI"],
        subcategories: {
          AI: ["Language Models", "Root Nodes"],
          "Language Models": ["Chat Models (Recommended)"]
        },
        resources: {
          primaryDocumentation: [
            {
              url: "https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatanthropic/"
            }
          ]
        },
        alias: ["claude", "sonnet", "opus"]
      },
      inputs: [],
      outputs: [import_n8n_workflow.NodeConnectionTypes.AiLanguageModel],
      outputNames: ["Model"],
      credentials: [
        {
          name: "anthropicApi",
          required: true
        }
      ],
      properties: [
        (0, import_sharedFields.getConnectionHintNoticeField)([import_n8n_workflow.NodeConnectionTypes.AiChain, import_n8n_workflow.NodeConnectionTypes.AiChain]),
        {
          ...modelField,
          displayOptions: {
            show: {
              "@version": [1]
            }
          }
        },
        {
          ...modelField,
          default: "claude-3-sonnet-20240229",
          displayOptions: {
            show: {
              "@version": [1.1]
            }
          }
        },
        {
          ...modelField,
          default: "claude-3-5-sonnet-20240620",
          options: (modelField.options ?? []).filter(
            (o) => "name" in o && !o.name.toString().startsWith("LEGACY")
          ),
          displayOptions: {
            show: {
              "@version": [{ _cnd: { lte: 1.2 } }]
            }
          }
        },
        {
          displayName: "Model",
          name: "model",
          type: "resourceLocator",
          default: {
            mode: "list",
            value: "claude-sonnet-4-20250514",
            cachedResultName: "Claude 4 Sonnet"
          },
          required: true,
          modes: [
            {
              displayName: "From List",
              name: "list",
              type: "list",
              placeholder: "Select a model...",
              typeOptions: {
                searchListMethod: "searchModels",
                searchable: true
              }
            },
            {
              displayName: "ID",
              name: "id",
              type: "string",
              placeholder: "Claude Sonnet"
            }
          ],
          description: 'The model. Choose from the list, or specify an ID. <a href="https://docs.anthropic.com/claude/docs/models-overview">Learn more</a>.',
          displayOptions: {
            show: {
              "@version": [{ _cnd: { gte: 1.3 } }]
            }
          }
        },
        {
          displayName: "Options",
          name: "options",
          placeholder: "Add Option",
          description: "Additional options to add",
          type: "collection",
          default: {},
          options: [
            {
              displayName: "Maximum Number of Tokens",
              name: "maxTokensToSample",
              default: DEFAULT_MAX_TOKENS,
              description: "The maximum number of tokens to generate in the completion",
              type: "number"
            },
            {
              displayName: "Sampling Temperature",
              name: "temperature",
              default: 0.7,
              typeOptions: { maxValue: 1, minValue: 0, numberPrecision: 1 },
              description: "Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
              type: "number",
              displayOptions: {
                hide: {
                  thinking: [true]
                }
              }
            },
            {
              displayName: "Top K",
              name: "topK",
              default: -1,
              typeOptions: { maxValue: 1, minValue: -1, numberPrecision: 1 },
              description: 'Used to remove "long tail" low probability responses. Defaults to -1, which disables it.',
              type: "number",
              displayOptions: {
                hide: {
                  thinking: [true]
                }
              }
            },
            {
              displayName: "Top P",
              name: "topP",
              default: 1,
              typeOptions: { maxValue: 1, minValue: 0, numberPrecision: 1 },
              description: "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.",
              type: "number",
              displayOptions: {
                hide: {
                  thinking: [true]
                }
              }
            },
            {
              displayName: "Enable Thinking",
              name: "thinking",
              type: "boolean",
              default: false,
              description: "Whether to enable thinking mode for the model"
            },
            {
              displayName: "Thinking Budget (Tokens)",
              name: "thinkingBudget",
              type: "number",
              default: MIN_THINKING_BUDGET,
              description: "The maximum number of tokens to use for thinking",
              displayOptions: {
                show: {
                  thinking: [true]
                }
              }
            }
          ]
        }
      ]
    };
  }
  async supplyData(itemIndex) {
    const credentials = await this.getCredentials(
      "anthropicApi"
    );
    const baseURL = credentials.url ?? "https://api.anthropic.com";
    const version = this.getNode().typeVersion;
    const modelName = version >= 1.3 ? this.getNodeParameter("model.value", itemIndex) : this.getNodeParameter("model", itemIndex);
    const options = this.getNodeParameter("options", itemIndex, {});
    let invocationKwargs = {};
    const tokensUsageParser = (result) => {
      const usage = result?.llmOutput?.usage ?? {
        input_tokens: 0,
        output_tokens: 0
      };
      return {
        completionTokens: usage.output_tokens,
        promptTokens: usage.input_tokens,
        totalTokens: usage.input_tokens + usage.output_tokens
      };
    };
    if (options.thinking) {
      invocationKwargs = {
        thinking: {
          type: "enabled",
          // If thinking is enabled, we need to set a budget.
          // We fallback to 1024 as that is the minimum
          budget_tokens: options.thinkingBudget ?? MIN_THINKING_BUDGET
        },
        // The default Langchain max_tokens is -1 (no limit) but Anthropic requires a number
        // higher than budget_tokens
        max_tokens: options.maxTokensToSample ?? DEFAULT_MAX_TOKENS,
        // These need to be unset when thinking is enabled.
        // Because the invocationKwargs will override the model options
        // we can pass options to the model and then override them here
        top_k: void 0,
        top_p: void 0,
        temperature: void 0
      };
    }
    const model = new import_anthropic.ChatAnthropic({
      anthropicApiKey: credentials.apiKey,
      model: modelName,
      anthropicApiUrl: baseURL,
      maxTokens: options.maxTokensToSample,
      temperature: options.temperature,
      topK: options.topK,
      topP: options.topP,
      callbacks: [new import_N8nLlmTracing.N8nLlmTracing(this, { tokensUsageParser })],
      onFailedAttempt: (0, import_n8nLlmFailedAttemptHandler.makeN8nLlmFailedAttemptHandler)(this),
      invocationKwargs,
      clientOptions: {
        fetchOptions: {
          dispatcher: (0, import_httpProxyAgent.getProxyAgent)(baseURL)
        }
      }
    });
    if (options.topP === void 0) {
      delete model.topP;
    }
    if (options.topP !== void 0 && options.temperature === void 0) {
      delete model.temperature;
    }
    return {
      response: model
    };
  }
}
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  LmChatAnthropic
});
//# sourceMappingURL=LmChatAnthropic.node.js.map