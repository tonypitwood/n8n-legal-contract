"use strict";
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);
var LmChatOpenAi_node_exports = {};
__export(LmChatOpenAi_node_exports, {
  LmChatOpenAi: () => LmChatOpenAi
});
module.exports = __toCommonJS(LmChatOpenAi_node_exports);
var import_openai = require("@langchain/openai");
var import_n8n_workflow = require("n8n-workflow");
var import_httpProxyAgent = require("../../../utils/httpProxyAgent");
var import_sharedFields = require("../../../utils/sharedFields");
var import_loadModels = require("./methods/loadModels");
var import_error_handling = require("../../vendors/OpenAi/helpers/error-handling");
var import_n8nLlmFailedAttemptHandler = require("../n8nLlmFailedAttemptHandler");
var import_N8nLlmTracing = require("../N8nLlmTracing");
class LmChatOpenAi {
  constructor() {
    this.methods = {
      listSearch: {
        searchModels: import_loadModels.searchModels
      }
    };
    this.description = {
      displayName: "OpenAI Chat Model",
      name: "lmChatOpenAi",
      icon: { light: "file:openAiLight.svg", dark: "file:openAiLight.dark.svg" },
      group: ["transform"],
      version: [1, 1.1, 1.2],
      description: "For advanced usage with an AI chain",
      defaults: {
        name: "OpenAI Chat Model"
      },
      codex: {
        categories: ["AI"],
        subcategories: {
          AI: ["Language Models", "Root Nodes"],
          "Language Models": ["Chat Models (Recommended)"]
        },
        resources: {
          primaryDocumentation: [
            {
              url: "https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatopenai/"
            }
          ]
        }
      },
      inputs: [],
      outputs: [import_n8n_workflow.NodeConnectionTypes.AiLanguageModel],
      outputNames: ["Model"],
      credentials: [
        {
          name: "openAiApi",
          required: true
        }
      ],
      requestDefaults: {
        ignoreHttpStatusErrors: true,
        baseURL: '={{ $parameter.options?.baseURL?.split("/").slice(0,-1).join("/") || $credentials?.url?.split("/").slice(0,-1).join("/") || "https://api.openai.com" }}'
      },
      properties: [
        (0, import_sharedFields.getConnectionHintNoticeField)([import_n8n_workflow.NodeConnectionTypes.AiChain, import_n8n_workflow.NodeConnectionTypes.AiAgent]),
        {
          displayName: 'If using JSON response format, you must include word "json" in the prompt in your chain or agent. Also, make sure to select latest models released post November 2023.',
          name: "notice",
          type: "notice",
          default: "",
          displayOptions: {
            show: {
              "/options.responseFormat": ["json_object"]
            }
          }
        },
        {
          displayName: "Model",
          name: "model",
          type: "options",
          description: 'The model which will generate the completion. <a href="https://beta.openai.com/docs/models/overview">Learn more</a>.',
          typeOptions: {
            loadOptions: {
              routing: {
                request: {
                  method: "GET",
                  url: '={{ $parameter.options?.baseURL?.split("/").slice(-1).pop() || $credentials?.url?.split("/").slice(-1).pop() || "v1" }}/models'
                },
                output: {
                  postReceive: [
                    {
                      type: "rootProperty",
                      properties: {
                        property: "data"
                      }
                    },
                    {
                      type: "filter",
                      properties: {
                        // If the baseURL is not set or is set to api.openai.com, include only chat models
                        pass: `={{
												($parameter.options?.baseURL && !$parameter.options?.baseURL?.startsWith('https://api.openai.com/')) ||
												($credentials?.url && !$credentials.url.startsWith('https://api.openai.com/')) ||
												$responseItem.id.startsWith('ft:') ||
												$responseItem.id.startsWith('o1') ||
												$responseItem.id.startsWith('o3') ||
												($responseItem.id.startsWith('gpt-') && !$responseItem.id.includes('instruct'))
											}}`
                      }
                    },
                    {
                      type: "setKeyValue",
                      properties: {
                        name: "={{$responseItem.id}}",
                        value: "={{$responseItem.id}}"
                      }
                    },
                    {
                      type: "sort",
                      properties: {
                        key: "name"
                      }
                    }
                  ]
                }
              }
            }
          },
          routing: {
            send: {
              type: "body",
              property: "model"
            }
          },
          default: "gpt-4o-mini",
          displayOptions: {
            hide: {
              "@version": [{ _cnd: { gte: 1.2 } }]
            }
          }
        },
        {
          displayName: "Model",
          name: "model",
          type: "resourceLocator",
          default: { mode: "list", value: "gpt-4.1-mini" },
          required: true,
          modes: [
            {
              displayName: "From List",
              name: "list",
              type: "list",
              placeholder: "Select a model...",
              typeOptions: {
                searchListMethod: "searchModels",
                searchable: true
              }
            },
            {
              displayName: "ID",
              name: "id",
              type: "string",
              placeholder: "gpt-4.1-mini"
            }
          ],
          description: "The model. Choose from the list, or specify an ID.",
          displayOptions: {
            hide: {
              "@version": [{ _cnd: { lte: 1.1 } }]
            }
          }
        },
        {
          displayName: 'When using non-OpenAI models via "Base URL" override, not all models might be chat-compatible or support other features, like tools calling or JSON response format',
          name: "notice",
          type: "notice",
          default: "",
          displayOptions: {
            show: {
              "/options.baseURL": [{ _cnd: { exists: true } }]
            }
          }
        },
        {
          displayName: "Options",
          name: "options",
          placeholder: "Add Option",
          description: "Additional options to add",
          type: "collection",
          default: {},
          options: [
            {
              displayName: "Base URL",
              name: "baseURL",
              default: "https://api.openai.com/v1",
              description: "Override the default base URL for the API",
              type: "string",
              displayOptions: {
                hide: {
                  "@version": [{ _cnd: { gte: 1.1 } }]
                }
              }
            },
            {
              displayName: "Frequency Penalty",
              name: "frequencyPenalty",
              default: 0,
              typeOptions: { maxValue: 2, minValue: -2, numberPrecision: 1 },
              description: "Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim",
              type: "number"
            },
            {
              displayName: "Maximum Number of Tokens",
              name: "maxTokens",
              default: -1,
              description: "The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).",
              type: "number",
              typeOptions: {
                maxValue: 32768
              }
            },
            {
              displayName: "Response Format",
              name: "responseFormat",
              default: "text",
              type: "options",
              options: [
                {
                  name: "Text",
                  value: "text",
                  description: "Regular text response"
                },
                {
                  name: "JSON",
                  value: "json_object",
                  description: "Enables JSON mode, which should guarantee the message the model generates is valid JSON"
                }
              ]
            },
            {
              displayName: "Presence Penalty",
              name: "presencePenalty",
              default: 0,
              typeOptions: { maxValue: 2, minValue: -2, numberPrecision: 1 },
              description: "Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics",
              type: "number"
            },
            {
              displayName: "Sampling Temperature",
              name: "temperature",
              default: 0.7,
              typeOptions: { maxValue: 2, minValue: 0, numberPrecision: 1 },
              description: "Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
              type: "number"
            },
            {
              displayName: "Reasoning Effort",
              name: "reasoningEffort",
              default: "medium",
              description: 'Controls the amount of reasoning tokens to use. A value of "low" will favor speed and economical token usage, "high" will favor more complete reasoning at the cost of more tokens generated and slower responses.',
              type: "options",
              options: [
                {
                  name: "Low",
                  value: "low",
                  description: "Favors speed and economical token usage"
                },
                {
                  name: "Medium",
                  value: "medium",
                  description: "Balance between speed and reasoning accuracy"
                },
                {
                  name: "High",
                  value: "high",
                  description: "Favors more complete reasoning at the cost of more tokens generated and slower responses"
                }
              ],
              displayOptions: {
                show: {
                  // reasoning_effort is only available on o1, o1-versioned, or on o3-mini and beyond, and gpt-5 models. Not on o1-mini or other GPT-models.
                  "/model": [{ _cnd: { regex: "(^o1([-\\d]+)?$)|(^o[3-9].*)|(^gpt-5.*)" } }]
                }
              }
            },
            {
              displayName: "Timeout",
              name: "timeout",
              default: 6e4,
              description: "Maximum amount of time a request is allowed to take in milliseconds",
              type: "number"
            },
            {
              displayName: "Max Retries",
              name: "maxRetries",
              default: 2,
              description: "Maximum number of retries to attempt",
              type: "number"
            },
            {
              displayName: "Top P",
              name: "topP",
              default: 1,
              typeOptions: { maxValue: 1, minValue: 0, numberPrecision: 1 },
              description: "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.",
              type: "number"
            }
          ]
        }
      ]
    };
  }
  async supplyData(itemIndex) {
    const credentials = await this.getCredentials("openAiApi");
    const version = this.getNode().typeVersion;
    const modelName = version >= 1.2 ? this.getNodeParameter("model.value", itemIndex) : this.getNodeParameter("model", itemIndex);
    const options = this.getNodeParameter("options", itemIndex, {});
    const configuration = {};
    if (options.baseURL) {
      configuration.baseURL = options.baseURL;
    } else if (credentials.url) {
      configuration.baseURL = credentials.url;
    }
    if (configuration.baseURL) {
      configuration.fetchOptions = {
        dispatcher: (0, import_httpProxyAgent.getProxyAgent)(configuration.baseURL ?? "https://api.openai.com/v1")
      };
    }
    if (credentials.header && typeof credentials.headerName === "string" && credentials.headerName && typeof credentials.headerValue === "string") {
      configuration.defaultHeaders = {
        [credentials.headerName]: credentials.headerValue
      };
    }
    const modelKwargs = {};
    if (options.responseFormat) modelKwargs.response_format = { type: options.responseFormat };
    if (options.reasoningEffort && ["low", "medium", "high"].includes(options.reasoningEffort))
      modelKwargs.reasoning_effort = options.reasoningEffort;
    const model = new import_openai.ChatOpenAI({
      apiKey: credentials.apiKey,
      model: modelName,
      ...options,
      timeout: options.timeout ?? 6e4,
      maxRetries: options.maxRetries ?? 2,
      configuration,
      callbacks: [new import_N8nLlmTracing.N8nLlmTracing(this)],
      modelKwargs,
      onFailedAttempt: (0, import_n8nLlmFailedAttemptHandler.makeN8nLlmFailedAttemptHandler)(this, import_error_handling.openAiFailedAttemptHandler)
    });
    return {
      response: model
    };
  }
}
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  LmChatOpenAi
});
//# sourceMappingURL=LmChatOpenAi.node.js.map