{"version":3,"sources":["../../../../../nodes/chains/ChainLLM/methods/chainExecutor.ts"],"sourcesContent":["import type { BaseLanguageModel } from '@langchain/core/language_models/base';\nimport type { BaseLLMOutputParser } from '@langchain/core/output_parsers';\nimport { JsonOutputParser, StringOutputParser } from '@langchain/core/output_parsers';\nimport type { ChatPromptTemplate, PromptTemplate } from '@langchain/core/prompts';\nimport type { IExecuteFunctions } from 'n8n-workflow';\n\nimport { getTracingConfig } from '@utils/tracing';\n\nimport { createPromptTemplate } from './promptUtils';\nimport type { ChainExecutionParams } from './types';\n\nexport class NaiveJsonOutputParser<\n\tT extends Record<string, any> = Record<string, any>,\n> extends JsonOutputParser<T> {\n\tasync parse(text: string): Promise<T> {\n\t\t// First try direct JSON parsing\n\t\ttry {\n\t\t\tconst directParsed = JSON.parse(text);\n\t\t\treturn directParsed as T;\n\t\t} catch (e) {\n\t\t\t// If fails, fall back to JsonOutputParser parser\n\t\t\treturn await super.parse(text);\n\t\t}\n\t}\n}\n\n/**\n * Type guard to check if the LLM has a modelKwargs property(OpenAI)\n */\nexport function isModelWithResponseFormat(\n\tllm: BaseLanguageModel,\n): llm is BaseLanguageModel & { modelKwargs: { response_format: { type: string } } } {\n\treturn (\n\t\t'modelKwargs' in llm &&\n\t\t!!llm.modelKwargs &&\n\t\ttypeof llm.modelKwargs === 'object' &&\n\t\t'response_format' in llm.modelKwargs\n\t);\n}\n\nexport function isModelInThinkingMode(\n\tllm: BaseLanguageModel,\n): llm is BaseLanguageModel & { lc_kwargs: { invocationKwargs: { thinking: { type: string } } } } {\n\treturn (\n\t\t'lc_kwargs' in llm &&\n\t\t'invocationKwargs' in llm.lc_kwargs &&\n\t\ttypeof llm.lc_kwargs.invocationKwargs === 'object' &&\n\t\t'thinking' in llm.lc_kwargs.invocationKwargs &&\n\t\tllm.lc_kwargs.invocationKwargs.thinking.type === 'enabled'\n\t);\n}\n\n/**\n * Type guard to check if the LLM has a format property(Ollama)\n */\nexport function isModelWithFormat(\n\tllm: BaseLanguageModel,\n): llm is BaseLanguageModel & { format: string } {\n\treturn 'format' in llm && typeof llm.format !== 'undefined';\n}\n\n/**\n * Determines if an LLM is configured to output JSON and returns the appropriate output parser\n */\nexport function getOutputParserForLLM(\n\tllm: BaseLanguageModel,\n): BaseLLMOutputParser<string | Record<string, unknown>> {\n\tif (isModelWithResponseFormat(llm) && llm.modelKwargs?.response_format?.type === 'json_object') {\n\t\treturn new NaiveJsonOutputParser();\n\t}\n\n\tif (isModelWithFormat(llm) && llm.format === 'json') {\n\t\treturn new NaiveJsonOutputParser();\n\t}\n\n\tif (isModelInThinkingMode(llm)) {\n\t\treturn new NaiveJsonOutputParser();\n\t}\n\n\treturn new StringOutputParser();\n}\n\n/**\n * Creates a simple chain for LLMs without output parsers\n */\nasync function executeSimpleChain({\n\tcontext,\n\tllm,\n\tquery,\n\tprompt,\n\tfallbackLlm,\n}: {\n\tcontext: IExecuteFunctions;\n\tllm: BaseLanguageModel;\n\tquery: string;\n\tprompt: ChatPromptTemplate | PromptTemplate;\n\tfallbackLlm?: BaseLanguageModel | null;\n}) {\n\tconst outputParser = getOutputParserForLLM(llm);\n\tlet model;\n\n\tif (fallbackLlm) {\n\t\tmodel = llm.withFallbacks([fallbackLlm]);\n\t} else {\n\t\tmodel = llm;\n\t}\n\n\tconst chain = prompt.pipe(model).pipe(outputParser).withConfig(getTracingConfig(context));\n\n\t// Execute the chain\n\tconst response = await chain.invoke({\n\t\tquery,\n\t\tsignal: context.getExecutionCancelSignal(),\n\t});\n\n\t// Ensure response is always returned as an array\n\treturn [response];\n}\n\n/**\n * Creates and executes an LLM chain with the given prompt and optional output parsers\n */\nexport async function executeChain({\n\tcontext,\n\titemIndex,\n\tquery,\n\tllm,\n\toutputParser,\n\tmessages,\n\tfallbackLlm,\n}: ChainExecutionParams): Promise<unknown[]> {\n\t// If no output parsers provided, use a simple chain with basic prompt template\n\tif (!outputParser) {\n\t\tconst promptTemplate = await createPromptTemplate({\n\t\t\tcontext,\n\t\t\titemIndex,\n\t\t\tllm,\n\t\t\tmessages,\n\t\t\tquery,\n\t\t});\n\n\t\treturn await executeSimpleChain({\n\t\t\tcontext,\n\t\t\tllm,\n\t\t\tquery,\n\t\t\tprompt: promptTemplate,\n\t\t\tfallbackLlm,\n\t\t});\n\t}\n\n\tconst formatInstructions = outputParser.getFormatInstructions();\n\n\t// Create a prompt template with format instructions\n\tconst promptWithInstructions = await createPromptTemplate({\n\t\tcontext,\n\t\titemIndex,\n\t\tllm,\n\t\tmessages,\n\t\tformatInstructions,\n\t\tquery,\n\t});\n\n\tconst chain = promptWithInstructions\n\t\t.pipe(llm)\n\t\t.pipe(outputParser)\n\t\t.withConfig(getTracingConfig(context));\n\tconst response = await chain.invoke({ query }, { signal: context.getExecutionCancelSignal() });\n\n\t// Ensure response is always returned as an array\n\t// eslint-disable-next-line @typescript-eslint/no-unsafe-return\n\treturn Array.isArray(response) ? response : [response];\n}\n"],"mappings":";;;;;;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEA,4BAAqD;AAIrD,qBAAiC;AAEjC,yBAAqC;AAG9B,MAAM,8BAEH,uCAAoB;AAAA,EAC7B,MAAM,MAAM,MAA0B;AAErC,QAAI;AACH,YAAM,eAAe,KAAK,MAAM,IAAI;AACpC,aAAO;AAAA,IACR,SAAS,GAAG;AAEX,aAAO,MAAM,MAAM,MAAM,IAAI;AAAA,IAC9B;AAAA,EACD;AACD;AAKO,SAAS,0BACf,KACoF;AACpF,SACC,iBAAiB,OACjB,CAAC,CAAC,IAAI,eACN,OAAO,IAAI,gBAAgB,YAC3B,qBAAqB,IAAI;AAE3B;AAEO,SAAS,sBACf,KACiG;AACjG,SACC,eAAe,OACf,sBAAsB,IAAI,aAC1B,OAAO,IAAI,UAAU,qBAAqB,YAC1C,cAAc,IAAI,UAAU,oBAC5B,IAAI,UAAU,iBAAiB,SAAS,SAAS;AAEnD;AAKO,SAAS,kBACf,KACgD;AAChD,SAAO,YAAY,OAAO,OAAO,IAAI,WAAW;AACjD;AAKO,SAAS,sBACf,KACwD;AACxD,MAAI,0BAA0B,GAAG,KAAK,IAAI,aAAa,iBAAiB,SAAS,eAAe;AAC/F,WAAO,IAAI,sBAAsB;AAAA,EAClC;AAEA,MAAI,kBAAkB,GAAG,KAAK,IAAI,WAAW,QAAQ;AACpD,WAAO,IAAI,sBAAsB;AAAA,EAClC;AAEA,MAAI,sBAAsB,GAAG,GAAG;AAC/B,WAAO,IAAI,sBAAsB;AAAA,EAClC;AAEA,SAAO,IAAI,yCAAmB;AAC/B;AAKA,eAAe,mBAAmB;AAAA,EACjC;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACD,GAMG;AACF,QAAM,eAAe,sBAAsB,GAAG;AAC9C,MAAI;AAEJ,MAAI,aAAa;AAChB,YAAQ,IAAI,cAAc,CAAC,WAAW,CAAC;AAAA,EACxC,OAAO;AACN,YAAQ;AAAA,EACT;AAEA,QAAM,QAAQ,OAAO,KAAK,KAAK,EAAE,KAAK,YAAY,EAAE,eAAW,iCAAiB,OAAO,CAAC;AAGxF,QAAM,WAAW,MAAM,MAAM,OAAO;AAAA,IACnC;AAAA,IACA,QAAQ,QAAQ,yBAAyB;AAAA,EAC1C,CAAC;AAGD,SAAO,CAAC,QAAQ;AACjB;AAKA,eAAsB,aAAa;AAAA,EAClC;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACD,GAA6C;AAE5C,MAAI,CAAC,cAAc;AAClB,UAAM,iBAAiB,UAAM,yCAAqB;AAAA,MACjD;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACD,CAAC;AAED,WAAO,MAAM,mBAAmB;AAAA,MAC/B;AAAA,MACA;AAAA,MACA;AAAA,MACA,QAAQ;AAAA,MACR;AAAA,IACD,CAAC;AAAA,EACF;AAEA,QAAM,qBAAqB,aAAa,sBAAsB;AAG9D,QAAM,yBAAyB,UAAM,yCAAqB;AAAA,IACzD;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACD,CAAC;AAED,QAAM,QAAQ,uBACZ,KAAK,GAAG,EACR,KAAK,YAAY,EACjB,eAAW,iCAAiB,OAAO,CAAC;AACtC,QAAM,WAAW,MAAM,MAAM,OAAO,EAAE,MAAM,GAAG,EAAE,QAAQ,QAAQ,yBAAyB,EAAE,CAAC;AAI7F,SAAO,MAAM,QAAQ,QAAQ,IAAI,WAAW,CAAC,QAAQ;AACtD;","names":[]}