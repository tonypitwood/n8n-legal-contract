{"version":3,"sources":["../../../../../nodes/chains/ChainLLM/methods/processItem.ts"],"sourcesContent":["import type { BaseLanguageModel } from '@langchain/core/language_models/base';\nimport { type IExecuteFunctions, NodeConnectionTypes, NodeOperationError } from 'n8n-workflow';\nimport assert from 'node:assert';\n\nimport { getPromptInputByType } from '@utils/helpers';\nimport { getOptionalOutputParser } from '@utils/output_parsers/N8nOutputParser';\n\nimport { executeChain } from './chainExecutor';\nimport { type MessageTemplate } from './types';\n\nasync function getChatModel(\n\tctx: IExecuteFunctions,\n\tindex: number = 0,\n): Promise<BaseLanguageModel | undefined> {\n\tconst connectedModels = await ctx.getInputConnectionData(NodeConnectionTypes.AiLanguageModel, 0);\n\n\tlet model;\n\n\tif (Array.isArray(connectedModels) && index !== undefined) {\n\t\tif (connectedModels.length <= index) {\n\t\t\treturn undefined;\n\t\t}\n\t\t// We get the models in reversed order from the workflow so we need to reverse them again to match the right index\n\t\tconst reversedModels = [...connectedModels].reverse();\n\t\tmodel = reversedModels[index] as BaseLanguageModel;\n\t} else {\n\t\tmodel = connectedModels as BaseLanguageModel;\n\t}\n\n\treturn model;\n}\n\nexport const processItem = async (ctx: IExecuteFunctions, itemIndex: number) => {\n\tconst needsFallback = ctx.getNodeParameter('needsFallback', 0, false) as boolean;\n\tconst llm = await getChatModel(ctx, 0);\n\tassert(llm, 'Please connect a model to the Chat Model input');\n\n\tconst fallbackLlm = needsFallback ? await getChatModel(ctx, 1) : null;\n\tif (needsFallback && !fallbackLlm) {\n\t\tthrow new NodeOperationError(\n\t\t\tctx.getNode(),\n\t\t\t'Please connect a model to the Fallback Model input or disable the fallback option',\n\t\t);\n\t}\n\n\t// Get output parser if configured\n\tconst outputParser = await getOptionalOutputParser(ctx, itemIndex);\n\n\t// Get user prompt based on node version\n\tlet prompt: string;\n\n\tif (ctx.getNode().typeVersion <= 1.3) {\n\t\tprompt = ctx.getNodeParameter('prompt', itemIndex) as string;\n\t} else {\n\t\tprompt = getPromptInputByType({\n\t\t\tctx,\n\t\t\ti: itemIndex,\n\t\t\tinputKey: 'text',\n\t\t\tpromptTypeKey: 'promptType',\n\t\t});\n\t}\n\n\t// Validate prompt\n\tif (prompt === undefined) {\n\t\tthrow new NodeOperationError(ctx.getNode(), \"The 'prompt' parameter is empty.\");\n\t}\n\n\t// Get chat messages if configured\n\tconst messages = ctx.getNodeParameter(\n\t\t'messages.messageValues',\n\t\titemIndex,\n\t\t[],\n\t) as MessageTemplate[];\n\n\t// Execute the chain\n\treturn await executeChain({\n\t\tcontext: ctx,\n\t\titemIndex,\n\t\tquery: prompt,\n\t\tllm,\n\t\toutputParser,\n\t\tmessages,\n\t\tfallbackLlm,\n\t});\n};\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;AACA,0BAAgF;AAChF,yBAAmB;AAEnB,qBAAqC;AACrC,6BAAwC;AAExC,2BAA6B;AAG7B,eAAe,aACd,KACA,QAAgB,GACyB;AACzC,QAAM,kBAAkB,MAAM,IAAI,uBAAuB,wCAAoB,iBAAiB,CAAC;AAE/F,MAAI;AAEJ,MAAI,MAAM,QAAQ,eAAe,KAAK,UAAU,QAAW;AAC1D,QAAI,gBAAgB,UAAU,OAAO;AACpC,aAAO;AAAA,IACR;AAEA,UAAM,iBAAiB,CAAC,GAAG,eAAe,EAAE,QAAQ;AACpD,YAAQ,eAAe,KAAK;AAAA,EAC7B,OAAO;AACN,YAAQ;AAAA,EACT;AAEA,SAAO;AACR;AAEO,MAAM,cAAc,OAAO,KAAwB,cAAsB;AAC/E,QAAM,gBAAgB,IAAI,iBAAiB,iBAAiB,GAAG,KAAK;AACpE,QAAM,MAAM,MAAM,aAAa,KAAK,CAAC;AACrC,yBAAAA,SAAO,KAAK,gDAAgD;AAE5D,QAAM,cAAc,gBAAgB,MAAM,aAAa,KAAK,CAAC,IAAI;AACjE,MAAI,iBAAiB,CAAC,aAAa;AAClC,UAAM,IAAI;AAAA,MACT,IAAI,QAAQ;AAAA,MACZ;AAAA,IACD;AAAA,EACD;AAGA,QAAM,eAAe,UAAM,gDAAwB,KAAK,SAAS;AAGjE,MAAI;AAEJ,MAAI,IAAI,QAAQ,EAAE,eAAe,KAAK;AACrC,aAAS,IAAI,iBAAiB,UAAU,SAAS;AAAA,EAClD,OAAO;AACN,iBAAS,qCAAqB;AAAA,MAC7B;AAAA,MACA,GAAG;AAAA,MACH,UAAU;AAAA,MACV,eAAe;AAAA,IAChB,CAAC;AAAA,EACF;AAGA,MAAI,WAAW,QAAW;AACzB,UAAM,IAAI,uCAAmB,IAAI,QAAQ,GAAG,kCAAkC;AAAA,EAC/E;AAGA,QAAM,WAAW,IAAI;AAAA,IACpB;AAAA,IACA;AAAA,IACA,CAAC;AAAA,EACF;AAGA,SAAO,UAAM,mCAAa;AAAA,IACzB,SAAS;AAAA,IACT;AAAA,IACA,OAAO;AAAA,IACP;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACD,CAAC;AACF;","names":["assert"]}