{"version":3,"sources":["../../../../../../nodes/vector_store/shared/createVectorStoreNode/operations/insertOperation.ts"],"sourcesContent":["import type { Document } from '@langchain/core/documents';\nimport type { Embeddings } from '@langchain/core/embeddings';\nimport type { VectorStore } from '@langchain/core/vectorstores';\nimport type { IExecuteFunctions, INodeExecutionData } from 'n8n-workflow';\nimport { NodeConnectionTypes } from 'n8n-workflow';\n\nimport { logAiEvent } from '@utils/helpers';\nimport type { N8nBinaryLoader } from '@utils/N8nBinaryLoader';\nimport type { N8nJsonLoader } from '@utils/N8nJsonLoader';\n\nimport { processDocument } from '../../processDocuments';\nimport type { VectorStoreNodeConstructorArgs } from '../types';\n\n/**\n * Handles the 'insert' operation mode\n * Inserts documents from the input into the vector store\n */\nexport async function handleInsertOperation<T extends VectorStore = VectorStore>(\n\tcontext: IExecuteFunctions,\n\targs: VectorStoreNodeConstructorArgs<T>,\n\tembeddings: Embeddings,\n): Promise<INodeExecutionData[]> {\n\tconst nodeVersion = context.getNode().typeVersion;\n\t// Get the input items and document data\n\tconst items = context.getInputData();\n\tconst documentInput = (await context.getInputConnectionData(NodeConnectionTypes.AiDocument, 0)) as\n\t\t| N8nJsonLoader\n\t\t| N8nBinaryLoader\n\t\t| Array<Document<Record<string, unknown>>>;\n\n\tconst resultData: INodeExecutionData[] = [];\n\tconst documentsForEmbedding: Array<Document<Record<string, unknown>>> = [];\n\n\t// Process each input item\n\tfor (let itemIndex = 0; itemIndex < items.length; itemIndex++) {\n\t\t// Check if execution is being cancelled\n\t\tif (context.getExecutionCancelSignal()?.aborted) {\n\t\t\tbreak;\n\t\t}\n\n\t\tconst itemData = items[itemIndex];\n\n\t\t// Process the document from the input\n\t\tconst processedDocuments = await processDocument(documentInput, itemData, itemIndex);\n\n\t\t// Add the serialized documents to the result\n\t\tresultData.push(...processedDocuments.serializedDocuments);\n\n\t\t// Add the processed documents to the documents to embedd\n\t\tdocumentsForEmbedding.push(...processedDocuments.processedDocuments);\n\n\t\t// For the version 1, we run the populateVectorStore(embedding and insert) function for each item\n\t\tif (nodeVersion === 1) {\n\t\t\tawait args.populateVectorStore(\n\t\t\t\tcontext,\n\t\t\t\tembeddings,\n\t\t\t\tprocessedDocuments.processedDocuments,\n\t\t\t\titemIndex,\n\t\t\t);\n\t\t}\n\t\t// Log the AI event for analytics\n\t\tlogAiEvent(context, 'ai-vector-store-populated');\n\t}\n\n\t// For the version 1.1, we run the populateVectorStore in batches\n\tif (nodeVersion >= 1.1) {\n\t\tconst embeddingBatchSize =\n\t\t\t(context.getNodeParameter('embeddingBatchSize', 0, 200) as number) ?? 200;\n\n\t\t// Populate the vector store with the processed documents in batches\n\t\tfor (let i = 0; i < documentsForEmbedding.length; i += embeddingBatchSize) {\n\t\t\tconst nextBatch = documentsForEmbedding.slice(i, i + embeddingBatchSize);\n\t\t\tawait args.populateVectorStore(context, embeddings, nextBatch, 0);\n\t\t}\n\t}\n\n\treturn resultData;\n}\n"],"mappings":";;;;;;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;AAIA,0BAAoC;AAEpC,qBAA2B;AAI3B,8BAAgC;AAOhC,eAAsB,sBACrB,SACA,MACA,YACgC;AAChC,QAAM,cAAc,QAAQ,QAAQ,EAAE;AAEtC,QAAM,QAAQ,QAAQ,aAAa;AACnC,QAAM,gBAAiB,MAAM,QAAQ,uBAAuB,wCAAoB,YAAY,CAAC;AAK7F,QAAM,aAAmC,CAAC;AAC1C,QAAM,wBAAkE,CAAC;AAGzE,WAAS,YAAY,GAAG,YAAY,MAAM,QAAQ,aAAa;AAE9D,QAAI,QAAQ,yBAAyB,GAAG,SAAS;AAChD;AAAA,IACD;AAEA,UAAM,WAAW,MAAM,SAAS;AAGhC,UAAM,qBAAqB,UAAM,yCAAgB,eAAe,UAAU,SAAS;AAGnF,eAAW,KAAK,GAAG,mBAAmB,mBAAmB;AAGzD,0BAAsB,KAAK,GAAG,mBAAmB,kBAAkB;AAGnE,QAAI,gBAAgB,GAAG;AACtB,YAAM,KAAK;AAAA,QACV;AAAA,QACA;AAAA,QACA,mBAAmB;AAAA,QACnB;AAAA,MACD;AAAA,IACD;AAEA,mCAAW,SAAS,2BAA2B;AAAA,EAChD;AAGA,MAAI,eAAe,KAAK;AACvB,UAAM,qBACJ,QAAQ,iBAAiB,sBAAsB,GAAG,GAAG,KAAgB;AAGvE,aAAS,IAAI,GAAG,IAAI,sBAAsB,QAAQ,KAAK,oBAAoB;AAC1E,YAAM,YAAY,sBAAsB,MAAM,GAAG,IAAI,kBAAkB;AACvE,YAAM,KAAK,oBAAoB,SAAS,YAAY,WAAW,CAAC;AAAA,IACjE;AAAA,EACD;AAEA,SAAO;AACR;","names":[]}