"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.createParameterUpdaterChain = exports.parametersSchema = void 0;
const messages_1 = require("@langchain/core/messages");
const prompts_1 = require("@langchain/core/prompts");
const zod_1 = require("zod");
const errors_1 = require("../errors");
const instance_url_1 = require("./prompts/instance-url");
const prompt_builder_1 = require("./prompts/prompt-builder");
exports.parametersSchema = zod_1.z
    .object({
    parameters: zod_1.z
        .object({})
        .passthrough()
        .describe("The complete updated parameters object for the node. This should be a JSON object that matches the node's parameter structure. Include ALL existing parameters plus the requested changes."),
})
    .describe('The complete updated parameters object for the node. Must include only parameters from <node_properties_definition>, for example For example: { "parameters": { "method": "POST", "url": "https://api.example.com", "sendHeaders": true, "headerParameters": { "parameters": [{ "name": "Content-Type", "value": "application/json" }] } } }}');
const nodeDefinitionPrompt = `
The node accepts these properties:
<node_properties_definition>
{node_definition}
</node_properties_definition>`;
const workflowContextPrompt = `
<current_workflow_json>
{workflow_json}
</current_workflow_json>

<current_simplified_execution_data>
{execution_data}
</current_simplified_execution_data>

<current_execution_nodes_schemas>
{execution_schema}
</current_execution_nodes_schemas>

<selected_node>
Name: {node_name}
Type: {node_type}

Current Parameters: {current_parameters}
</selected_node>

<requested_changes>
{changes}
</requested_changes>

Based on the requested changes and the node's property definitions, return the complete updated parameters object.`;
const createParameterUpdaterChain = (llm, options, logger) => {
    if (typeof llm.withStructuredOutput !== 'function') {
        throw new errors_1.LLMServiceError("LLM doesn't support withStructuredOutput", {
            llmModel: llm._llmType(),
        });
    }
    const systemPromptContent = prompt_builder_1.ParameterUpdatePromptBuilder.buildSystemPrompt({
        nodeType: options.nodeType,
        nodeDefinition: options.nodeDefinition,
        requestedChanges: options.requestedChanges,
        hasResourceLocatorParams: prompt_builder_1.ParameterUpdatePromptBuilder.hasResourceLocatorParameters(options.nodeDefinition),
    });
    const tokenEstimate = prompt_builder_1.ParameterUpdatePromptBuilder.estimateTokens(systemPromptContent);
    logger?.debug(`Parameter updater prompt size: ~${tokenEstimate} tokens`);
    const systemPrompt = new messages_1.SystemMessage({
        content: [
            {
                type: 'text',
                text: systemPromptContent,
                cache_control: { type: 'ephemeral' },
            },
        ],
    });
    const nodeDefinitionMessage = prompts_1.ChatPromptTemplate.fromMessages([
        [
            'human',
            [
                {
                    type: 'text',
                    text: nodeDefinitionPrompt,
                    cache_control: { type: 'ephemeral' },
                },
                {
                    type: 'text',
                    text: instance_url_1.instanceUrlPrompt,
                },
            ],
        ],
    ]);
    const workflowContextMessage = prompts_1.HumanMessagePromptTemplate.fromTemplate(workflowContextPrompt);
    const prompt = prompts_1.ChatPromptTemplate.fromMessages([
        systemPrompt,
        nodeDefinitionMessage,
        workflowContextMessage,
    ]);
    const llmWithStructuredOutput = llm.withStructuredOutput(exports.parametersSchema);
    const modelWithStructure = prompt.pipe(llmWithStructuredOutput);
    return modelWithStructure;
};
exports.createParameterUpdaterChain = createParameterUpdaterChain;
//# sourceMappingURL=parameter-updater.js.map